# Getting started with Kafka 
Apache Kafka is an open-source distributed event streaming platform designed for high-throughput, fault-tolerant, and scalable data streaming.
Kafka is built around publish-subscribe model, allowing producers to send messages to topics, and consumers to subscribe to topics to receive those messages. 
**Topic:** Topics are logical channels in which messages are sent by producers, and from which messages are consumed by the consumers. Topics can receive messages of any format without any constraint, however, you cannot query from a topic directly, instead, we need to use consumers to read from any topic channel. Kafka topics are **immutable**, which means we cannot detele nor update 
**Partition:** Kafka topics can be split into partitions, which are the basic unit of parallelism and distribution. Partitions allow Kafka to scale and distribute data across multiple brokers.
**Broker:** A Kafka broker is a server that stores and manages the messages published to Kafka topics. Brokers collectively form a Kafka cluster. Producers publish messages to brokers, which then store those messages on disk. Consumers read messages from brokers. As mentioned above, each topic can be made up of multiple partitions, and each partition can be hosted within a broker, in which the messages are stored. 
**Offset:** Offset is a unique identifier within a message in a topic. Consummers keep track of their progress by storing the offset of the last message sent. Offsets have meanings only within a specific partition, meaning that offset 3 in partition 1, does not correspond to offset 3 in partition 2. So, the order of messages are reserved within a partition, but not across partitions. 
**Producers:** Producers write data to partitions within a topic. This means that the client application (producer) knows in advance on which partition it's sending the data to, and which message broker is accommodating that partition. Producers can choose to send messages along with a message key. If they provide this optional parameter, messages with similar keys will end up in the same partition, thanks to a hashing strategy. 
**Key & value:** In Kafka, each message consists of a key and a value. Both the key and value can be serialized separately. The key is typically used for partitioning messages, while the value carries the actual data.
**Kafka Message serielizer:** Serialization is a crucial aspect of Kafka's design because it ensures that messages can be efficiently transmitted and processed by Kafka brokers, producers, and consumers while maintaining compatibility and efficiency. Choosing the right serialization format for your use case and ensuring proper serialization and deserialization practices are essential for a smooth Kafka deployment. When a producer sends a message to a Kafka topic, it serializes the key and value from their native format into a binary format. Conversely, when a consumer receives a message, it deserializes the binary key and value back into their native format for processing. Kafka provides pluggable serializers and deserializers for various data formats. Common serialization formats include Avro, JSON, and Protobuf. You can also implement custom serializers to handle specific data formats. Proper message serialization can have a significant impact on Kafka's efficiency and performance. Efficient serialization reduces the size of messages, leading to lower storage and network transfer costs and faster message processing.